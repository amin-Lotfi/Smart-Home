{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/amin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لامپ پذیرایی رو روشن کن ** 1\n",
      "\n",
      "لامپ پذیرایی رو خاموش کن ** 2 \n",
      "\n",
      "لامپ آشپزخانه رو روشن کن ** 3\n",
      "\n",
      "لامپ آشپزخانه رو خاموش کن ** 4 \n",
      "\n",
      "لامپ حمام رو روشن کن ** 5\n",
      "\n",
      "لامپ حمام رو خاموش کن ** 6 \n",
      "\n",
      "لامپ دستشویی رو روشن کن ** 7\n",
      "\n",
      "لامپ دستشویی رو خاموش کن ** 8 \n",
      "\n",
      "لامپ دفتر رو روشن کن ** 9\n",
      "\n",
      "لامپ دفتر رو خاموش کن ** 0 \n",
      "\n",
      "آبیاری رو روشن کن **  H\n",
      "\n",
      "آبیاری رو خاموش کن ** L\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from hazm import Normalizer\n",
    "from langdetect import detect\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    data = []\n",
    "    labels = []\n",
    "    existing_questions = {} \n",
    "    for line in lines:\n",
    "        if \"**\" in line:  \n",
    "            print(line)\n",
    "            question, answer = line.strip().split(\"**\")\n",
    "            similar_question = None\n",
    "            for existing_question in existing_questions:\n",
    "                if fuzz.ratio(existing_question, question) > 80:  \n",
    "                    similar_question = existing_question\n",
    "                    break\n",
    "            if similar_question:\n",
    "                existing_questions[similar_question] += \" \" + answer\n",
    "            else:\n",
    "                existing_questions[question] = answer\n",
    "\n",
    "    for question, answer in existing_questions.items():\n",
    "        lang = detect(question)\n",
    "        if lang == 'fa':\n",
    "            tokenizer = word_tokenize\n",
    "            normalizer = Normalizer().normalize\n",
    "        else:\n",
    "            tokenizer = word_tokenize\n",
    "            normalizer = lambda x: x  \n",
    "        question_tokens = tokenizer(normalizer(question))\n",
    "        processed_question = ' '.join(question_tokens)\n",
    "        data.append(processed_question)\n",
    "        labels.append(answer)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "X, y = load_data('data.txt')\n",
    "\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "])\n",
    "\n",
    "X_tfidf = preprocessing_pipeline.fit_transform(X)\n",
    "svm_classifier = SVC(kernel='linear', probability=True)\n",
    "svm_classifier.fit(X_tfidf, y)\n",
    "joblib.dump((preprocessing_pipeline, svm_classifier), 'model.pkl')\n",
    "\n",
    "def predict_answer(question):\n",
    "    loaded_pipeline, loaded_classifier = joblib.load('model.pkl')\n",
    "    lang = detect(question)\n",
    "    if lang == 'fa':\n",
    "        tokenizer = word_tokenize\n",
    "        normalizer = Normalizer().normalize\n",
    "    else:\n",
    "        tokenizer = word_tokenize\n",
    "        normalizer = lambda x: x \n",
    "    question_tokens = tokenizer(normalizer(question))\n",
    "    processed_question = ' '.join(question_tokens)\n",
    "    question_tfidf = loaded_pipeline.transform([processed_question])\n",
    "    probabilities = loaded_classifier.predict_proba(question_tfidf)\n",
    "    max_probability_index = np.argmax(probabilities)\n",
    "    predicted_label = loaded_classifier.classes_[max_probability_index]\n",
    "    return predicted_label\n",
    "\n",
    "\n",
    "question = \"لامپ حمام رو روشن کن\"\n",
    "predicted_answer = predict_answer(question)\n",
    "\n",
    "print(\"Predicted answer:\", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/amin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer:  6  6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from hazm import Normalizer\n",
    "from langdetect import detect\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    data = []\n",
    "    labels = []\n",
    "    existing_questions = {} \n",
    "    for line in lines:\n",
    "        if \"**\" in line:  \n",
    "            question, answer = line.strip().split(\"**\")\n",
    "            similar_question = None\n",
    "            for existing_question in existing_questions:\n",
    "                if fuzz.ratio(existing_question, question) > 80:  \n",
    "                    similar_question = existing_question\n",
    "                    break\n",
    "            if similar_question:\n",
    "                existing_questions[similar_question] += \" \" + answer\n",
    "            else:\n",
    "                existing_questions[question] = answer\n",
    "\n",
    "    for question, answer in existing_questions.items():\n",
    "        lang = detect(question)\n",
    "        if lang == 'fa':\n",
    "            tokenizer = word_tokenize\n",
    "            normalizer = Normalizer().normalize\n",
    "        else:\n",
    "            tokenizer = word_tokenize\n",
    "            normalizer = lambda x: x  \n",
    "        question_tokens = tokenizer(normalizer(question))\n",
    "        processed_question = ' '.join(question_tokens)\n",
    "        data.append(processed_question)\n",
    "        labels.append(answer)\n",
    "    return data, labels\n",
    "\n",
    "X, y = load_data('data.txt')\n",
    "\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "])\n",
    "\n",
    "X_tfidf = preprocessing_pipeline.fit_transform(X)\n",
    "svm_classifier = SVC(kernel='linear', probability=True)\n",
    "svm_classifier.fit(X_tfidf, y)\n",
    "joblib.dump((preprocessing_pipeline, svm_classifier), 'model.pkl')\n",
    "\n",
    "def predict_answer(question):\n",
    "    loaded_pipeline, loaded_classifier = joblib.load('model.pkl')\n",
    "    lang = detect(question)\n",
    "    if lang == 'fa':\n",
    "        tokenizer = word_tokenize\n",
    "        normalizer = Normalizer().normalize\n",
    "    else:\n",
    "        tokenizer = word_tokenize\n",
    "        normalizer = lambda x: x \n",
    "    question_tokens = tokenizer(normalizer(question))\n",
    "    processed_question = ' '.join(question_tokens)\n",
    "    question_tfidf = loaded_pipeline.transform([processed_question])\n",
    "    probabilities = loaded_classifier.predict_proba(question_tfidf)\n",
    "    max_probability_index = np.argmax(probabilities)\n",
    "    predicted_label = loaded_classifier.classes_[max_probability_index]\n",
    "    return predicted_label\n",
    "\n",
    "question = \"آبیاری رو روشن کن\"\n",
    "predicted_answer = predict_answer(question)\n",
    "\n",
    "print(\"Predicted answer:\", predicted_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.0941 - loss: 2.4546 - val_accuracy: 0.1882 - val_loss: 2.1559\n",
      "Epoch 2/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.1813 - loss: 2.1719 - val_accuracy: 0.1798 - val_loss: 2.1043\n",
      "Epoch 3/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.1804 - loss: 2.0706 - val_accuracy: 0.2079 - val_loss: 2.0180\n",
      "Epoch 4/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.2107 - loss: 1.9785 - val_accuracy: 0.2725 - val_loss: 1.7992\n",
      "Epoch 5/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.3343 - loss: 1.7155 - val_accuracy: 0.2978 - val_loss: 1.6973\n",
      "Epoch 6/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.3515 - loss: 1.5858 - val_accuracy: 0.4354 - val_loss: 1.3077\n",
      "Epoch 7/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.4429 - loss: 1.2711 - val_accuracy: 0.5702 - val_loss: 1.0282\n",
      "Epoch 8/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.5179 - loss: 1.0127 - val_accuracy: 0.5281 - val_loss: 1.2371\n",
      "Epoch 9/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.4993 - loss: 1.2177 - val_accuracy: 0.5702 - val_loss: 1.0368\n",
      "Epoch 10/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.6618 - loss: 0.8606 - val_accuracy: 0.3202 - val_loss: 2.4506\n",
      "Epoch 11/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.3787 - loss: 1.9520 - val_accuracy: 0.6601 - val_loss: 1.0460\n",
      "Epoch 12/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.6031 - loss: 1.0073 - val_accuracy: 0.7247 - val_loss: 0.7993\n",
      "Epoch 13/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.7167 - loss: 0.7746 - val_accuracy: 0.7949 - val_loss: 0.4847\n",
      "Epoch 14/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.8100 - loss: 0.5071 - val_accuracy: 0.8062 - val_loss: 0.6288\n",
      "Epoch 15/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.8389 - loss: 0.4398 - val_accuracy: 0.7978 - val_loss: 0.6540\n",
      "Epoch 16/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.7954 - loss: 0.5870 - val_accuracy: 0.8483 - val_loss: 0.3647\n",
      "Epoch 17/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.8373 - loss: 0.3748 - val_accuracy: 0.9073 - val_loss: 0.2987\n",
      "Epoch 18/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.8905 - loss: 0.2841 - val_accuracy: 0.9129 - val_loss: 0.2825\n",
      "Epoch 19/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.8815 - loss: 0.3488 - val_accuracy: 0.8652 - val_loss: 0.3967\n",
      "Epoch 20/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.8272 - loss: 0.4306 - val_accuracy: 0.9045 - val_loss: 0.3545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "data_file = \"data.txt\"\n",
    "with open(data_file, 'r', encoding='utf-8') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "input_texts = []\n",
    "output_labels = []\n",
    "for line in data:\n",
    "    if \"**\" in line:  \n",
    "        text, label = line.strip().split(\"**\")\n",
    "        input_texts.append(text.strip())\n",
    "        output_labels.append(label.strip())\n",
    "\n",
    "# Data Augmentation: Adding reversed sentences\n",
    "augmented_input_texts = input_texts + [text[::-1] for text in input_texts]\n",
    "augmented_output_labels = output_labels + output_labels\n",
    "\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(augmented_input_texts)\n",
    "num_chars = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = tokenizer.texts_to_sequences(augmented_input_texts)\n",
    "max_sequence_length = max(len(seq) for seq in input_sequences)\n",
    "padded_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "label_dict = {label: idx for idx, label in enumerate(np.unique(augmented_output_labels))}\n",
    "numerical_labels = [label_dict[label] for label in augmented_output_labels]\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=num_chars, output_dim=128),\n",
    "    Bidirectional(LSTM(500, return_sequences=True)),\n",
    "    LSTM(500),\n",
    "    Dense(len(label_dict), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(padded_sequences, np.array(numerical_labels), epochs=20, batch_size=128, validation_split=0.2)\n",
    "\n",
    "model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "0.991939\n",
      "Predicted label (Neural Network): lamp3_on\n"
     ]
    }
   ],
   "source": [
    "neural_network_model = load_model('model.h5')\n",
    "\n",
    "new_text = \" لامپ حموم رو روشن کن\"\n",
    "\n",
    "new_sequence = tokenizer.texts_to_sequences([new_text])\n",
    "new_padded_sequence = pad_sequences(new_sequence, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "predicted_label_index_nn = np.argmax(neural_network_model.predict(new_padded_sequence), axis=1)[0]\n",
    "predicted_label_nn = list(label_dict.keys())[predicted_label_index_nn]\n",
    "\n",
    "prediction_confidence = neural_network_model.predict(new_padded_sequence)[0][predicted_label_index_nn]\n",
    "print(prediction_confidence)\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "if prediction_confidence < confidence_threshold:\n",
    "    print(\"Low confidence prediction. Please check the request.\")\n",
    "else:\n",
    "    print(\"Predicted label (Neural Network):\", predicted_label_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label (SVM): lamp1_on\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from hazm import Normalizer\n",
    "\n",
    "\n",
    "data_file = \"data.txt\"\n",
    "with open(data_file, 'r', encoding='utf-8') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "input_texts = []\n",
    "output_labels = []\n",
    "for line in data:\n",
    "    if \"**\" in line:  \n",
    "        text, label = line.strip().split(\"**\")\n",
    "        input_texts.append(text.strip())\n",
    "        output_labels.append(label.strip())\n",
    "\n",
    "augmented_input_texts = input_texts + [text[::-1] for text in input_texts]\n",
    "augmented_output_labels = output_labels + output_labels\n",
    "\n",
    "normalizer = Normalizer()\n",
    "normalized_texts = [normalizer.normalize(text) for text in augmented_input_texts]\n",
    "\n",
    "model_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=word_tokenize)),\n",
    "    ('svc', SVC(probability=True))\n",
    "])\n",
    "\n",
    "model_pipeline.fit(normalized_texts, augmented_output_labels)\n",
    "\n",
    "new_text = \" لامپ پذیرایی  روشن کن\"\n",
    "\n",
    "normalized_new_text = normalizer.normalize(new_text)\n",
    "\n",
    "predicted_label_index_svm = model_pipeline.predict([normalized_new_text])[0]\n",
    "prediction_confidence = max(model_pipeline.predict_proba([normalized_new_text])[0])\n",
    "\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "if prediction_confidence < confidence_threshold:\n",
    "    print(\"Low confidence prediction. Please check the request.\")\n",
    "else:\n",
    "    print(\"Predicted label (SVM):\", predicted_label_index_svm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
